{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e96c0bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Basic\n",
    "pr_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pull_request.parquet\")\n",
    "repo_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/repository.parquet\")\n",
    "\n",
    "# Commits\n",
    "# pr_commits_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pr_commits.parquet\")\n",
    "pr_commit_details_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/pr_commit_details.parquet\")\n",
    "\n",
    "# Related issues\n",
    "# related_issue_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/related_issue.parquet\")\n",
    "# issue_df = pd.read_parquet(\"hf://datasets/hao-li/AIDev/issue.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0ba23331",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from typing import Optional\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Reuse a single session (connection pooling + slightly faster)\n",
    "_SESSION = requests.Session()\n",
    "\n",
    "def _to_api_repo_url(repo_url: str) -> str:\n",
    "    \"\"\"\n",
    "    Normalize a repo URL to GitHub API base:\n",
    "    - 'https://api.github.com/repos/owner/repo' -> unchanged\n",
    "    - 'https://github.com/owner/repo'           -> 'https://api.github.com/repos/owner/repo'\n",
    "    \"\"\"\n",
    "    repo_url = str(repo_url).rstrip(\"/\")\n",
    "    if \"api.github.com/repos/\" in repo_url:\n",
    "        return repo_url\n",
    "    m = re.match(r\"^https?://github\\.com/([^/]+)/([^/]+)$\", repo_url)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Unrecognized repo_url format: {repo_url}\")\n",
    "    owner, repo = m.group(1), m.group(2)\n",
    "    return f\"https://api.github.com/repos/{owner}/{repo}\"\n",
    "\n",
    "def _sleep_until(reset_epoch: int) -> None:\n",
    "    # reset header is unix seconds; add a 1s cushion\n",
    "    sleep_s = max(0, reset_epoch - int(time.time())) + 1\n",
    "    if sleep_s > 0:\n",
    "        print(f\"Sleeping for {sleep_s/60} minutes until rate limit reset...\")\n",
    "        time.sleep(sleep_s)\n",
    "\n",
    "def _request_with_rate_limit(url: str, headers: dict, timeout: int, max_retries: int = 5):\n",
    "    \"\"\"\n",
    "    GET with:\n",
    "      - X-RateLimit-* handling (sleep until reset when remaining==0)\n",
    "      - 429 / Retry-After handling\n",
    "      - transient 5xx / network errors with exponential backoff + jitter\n",
    "      - returns Response on success, None on 404, raises on final failure\n",
    "    \"\"\"\n",
    "    backoff = 1.0\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            resp = _SESSION.get(url, headers=headers, timeout=timeout)\n",
    "        except requests.RequestException:\n",
    "            # network hiccup → backoff and retry\n",
    "            time.sleep(backoff + random.random())\n",
    "            backoff = min(backoff * 2, 30)\n",
    "            continue\n",
    "\n",
    "        # 404: missing commit (deleted fork, etc.)\n",
    "        if resp.status_code == 404:\n",
    "            return None\n",
    "\n",
    "        # Hard rate limit: remaining==0 → sleep until reset, then retry\n",
    "        if resp.status_code == 403:\n",
    "            remaining = resp.headers.get(\"X-RateLimit-Remaining\")\n",
    "            reset = resp.headers.get(\"X-RateLimit-Reset\")\n",
    "            # Secondary rate limit sometimes has Retry-After\n",
    "            retry_after = resp.headers.get(\"Retry-After\")\n",
    "            if remaining is not None and remaining.isdigit() and int(remaining) == 0 and reset and reset.isdigit():\n",
    "                _sleep_until(int(reset))\n",
    "                continue\n",
    "            if retry_after and retry_after.isdigit():\n",
    "                time.sleep(int(retry_after))\n",
    "                continue\n",
    "            # Generic 403 (e.g., perms): break and raise\n",
    "            resp.raise_for_status()\n",
    "\n",
    "        # Too Many Requests\n",
    "        if resp.status_code == 429:\n",
    "            retry_after = resp.headers.get(\"Retry-After\")\n",
    "            time.sleep(int(retry_after) if retry_after and retry_after.isdigit() else 60)\n",
    "            continue\n",
    "\n",
    "        # Transient server errors → backoff\n",
    "        if resp.status_code in (502, 503, 504):\n",
    "            time.sleep(backoff + random.random())\n",
    "            backoff = min(backoff * 2, 30)\n",
    "            continue\n",
    "\n",
    "        # Success codes\n",
    "        if 200 <= resp.status_code < 300:\n",
    "            # Also check rate-limit headers and pause *after* a successful call\n",
    "            remaining = resp.headers.get(\"X-RateLimit-Remaining\")\n",
    "            reset = resp.headers.get(\"X-RateLimit-Reset\")\n",
    "            if remaining is not None and remaining.isdigit() and int(remaining) == 0 and reset and reset.isdigit():\n",
    "                # We consumed the last request in the window; sleep before returning\n",
    "                _sleep_until(int(reset))\n",
    "            return resp\n",
    "\n",
    "        # Other errors → raise (will exit loop)\n",
    "        resp.raise_for_status()\n",
    "\n",
    "    # If we exit loop without returning/raising, raise generic error\n",
    "    raise RuntimeError(f\"Failed GET after {max_retries} attempts: {url}\")\n",
    "\n",
    "def fetch_commit_json(repo_url: str, sha: str, token: Optional[str] = None, timeout: int = 30) -> dict | None:\n",
    "    \"\"\"\n",
    "    GET the commit JSON from GitHub.\n",
    "    Returns parsed JSON dict, or None on 404.\n",
    "    Raises for other HTTP errors after retries.\n",
    "    \"\"\"\n",
    "    api_repo = _to_api_repo_url(repo_url)  # normalize to API base safely\n",
    "    url = f\"{api_repo}/commits/{sha}\"\n",
    "    headers = {\n",
    "        \"Accept\": \"application/vnd.github+json\",\n",
    "        \"User-Agent\": \"pr-commit-details-fetch/1.1\",\n",
    "    }\n",
    "    token = token or os.getenv(\"GITHUB_TOKEN\")\n",
    "    if token:\n",
    "        headers[\"Authorization\"] = f\"Bearer {token}\"\n",
    "\n",
    "    resp = _request_with_rate_limit(url, headers=headers, timeout=timeout, max_retries=5)\n",
    "    if resp is None:  # 404\n",
    "        print(f\"[404] Commit {sha} not found at {api_repo}\", flush=True)\n",
    "        return None\n",
    "    return resp.json()\n",
    "\n",
    "def fix_commits(broken_commit_details_pd: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For each row (expects columns: sha, repo_url, ...), fetch the commit JSON and\n",
    "    expand into one row per file. Returns a DataFrame with the SAME columns as input.\n",
    "    \"\"\"\n",
    "    token = os.getenv(\"GITHUB_TOKEN\")\n",
    "    if not token:\n",
    "        raise ValueError(\"Error: GITHUB_TOKEN not set in environment\")\n",
    "\n",
    "    # Validate required columns\n",
    "    required = {\"sha\", \"repo_url\"}\n",
    "    missing = required - set(broken_commit_details_pd.columns)\n",
    "    if missing:\n",
    "        raise ValueError(f\"Input is missing required columns: {missing}\")\n",
    "\n",
    "    fixed_rows = []\n",
    "    # faster & safer than iterrows (typed attributes, no dtype coercion)\n",
    "    for row in broken_commit_details_pd.itertuples(index=False):\n",
    "        sha = str(getattr(row, \"sha\"))\n",
    "        repo_url = str(getattr(row, \"repo_url\"))\n",
    "\n",
    "        try:\n",
    "            commit_json = fetch_commit_json(repo_url, sha, token=token)\n",
    "        except Exception as e:\n",
    "            # Log and continue; do not halt the whole batch\n",
    "            print(f\"[error] fetch_commit_json failed for {repo_url} {sha}: {e}\", flush=True)\n",
    "            continue\n",
    "\n",
    "        if not commit_json:\n",
    "            continue\n",
    "\n",
    "        files = (commit_json.get(\"files\") or [])\n",
    "        if not files:\n",
    "            # likely a merge commit; nothing to expand\n",
    "            continue\n",
    "\n",
    "        # Build a base dict from the row (all original columns)\n",
    "        base = {col: getattr(row, col) for col in broken_commit_details_pd.columns}\n",
    "\n",
    "        for f in files:\n",
    "            rec = base.copy()\n",
    "            rec[\"filename\"]  = f.get(\"filename\")\n",
    "            rec[\"status\"]    = f.get(\"status\")\n",
    "            rec[\"additions\"] = int(f.get(\"additions\") or 0)\n",
    "            rec[\"deletions\"] = int(f.get(\"deletions\") or 0)\n",
    "            rec[\"changes\"]   = int(f.get(\"changes\")  or 0)\n",
    "            rec[\"patch\"]     = f.get(\"patch\")\n",
    "            fixed_rows.append(rec)\n",
    "\n",
    "    return pd.DataFrame(fixed_rows, columns=broken_commit_details_pd.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "cd3b7cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "181511\n",
      "Unique commits to process: 0\n",
      "Done. Re-run any time; it will skip (pr_id, sha) pairs already in the CSV.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "CSV_PATH = \"fixed_commits.csv\"\n",
    "BATCH_SIZE = 100\n",
    "\n",
    "pr_core = pr_df.loc[:, [\"id\", \"repo_url\"]].drop_duplicates(\"id\")\n",
    "\n",
    "# left-join by PR id → adds repo_url to each file row\n",
    "pcd_with_repo = (\n",
    "    pr_commit_details_df\n",
    "    .merge(pr_core, left_on=\"pr_id\", right_on=\"id\", how=\"left\", validate=\"many_to_one\")\n",
    "    .drop(columns=[\"id\"])  # drop the duplicate PR id from pr_df\n",
    ")\n",
    "todo = pcd_with_repo[pcd_with_repo[\"filename\"].isna() & (pcd_with_repo[\"commit_stats_total\"]>0)].drop_duplicates(subset=['sha', 'pr_id'])\n",
    "\n",
    "if os.path.exists(CSV_PATH) and os.path.getsize(CSV_PATH) > 0:\n",
    "    done = pd.read_csv(CSV_PATH)\n",
    "    print(len(done))\n",
    "    # Build a simple key for set-membership; normalize to string to avoid dtype mismatches\n",
    "    done_keys = set(done[\"pr_id\"].astype(str) + \"|\" + done[\"sha\"].astype(str))\n",
    "    todo_key  = (todo[\"pr_id\"].astype(str) + \"|\" + todo[\"sha\"].astype(str))\n",
    "    todo      = todo.loc[~todo_key.isin(done_keys)].copy()\n",
    "\n",
    "print(f\"Unique commits to process: {len(todo):,}\")\n",
    "\n",
    "# 4) Helper to append without losing header on first write\n",
    "def _append_csv(df: pd.DataFrame, path: str) -> None:\n",
    "    exists = os.path.exists(path) and os.path.getsize(path) > 0\n",
    "    df.to_csv(path, mode=(\"a\" if exists else \"w\"), header=not exists, index=False)\n",
    "\n",
    "# 5) Stream through in batches of 100 and save progress each time\n",
    "processed = 0\n",
    "for start in range(0, len(todo), BATCH_SIZE):\n",
    "    batch = todo.iloc[start:start + BATCH_SIZE].copy()\n",
    "\n",
    "    # Optional guard: skip rows without a usable repo_url\n",
    "    if \"repo_url\" in batch.columns:\n",
    "        batch = batch[batch[\"repo_url\"].notna() & (batch[\"repo_url\"].astype(str).str.len() > 0)]\n",
    "    if batch.empty:\n",
    "        continue\n",
    "\n",
    "    # Your rate-limit–aware function that expands rows one-per-file\n",
    "    fixed = fix_commits(batch)\n",
    "\n",
    "    if not fixed.empty:\n",
    "        _append_csv(fixed, CSV_PATH)\n",
    "\n",
    "    processed += len(batch)\n",
    "    print(f\"[progress] {processed:,}/{len(todo):,} commits processed; wrote {len(fixed):,} rows to {CSV_PATH}\")\n",
    "\n",
    "print(\"Done. Re-run any time; it will skip (pr_id, sha) pairs already in the CSV.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47507fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
